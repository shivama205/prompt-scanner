#!/usr/bin/env python3
"""
Prompt Scanner Decorator Example

This script demonstrates how to use the PromptScanner decorators
to scan inputs and outputs for unsafe content.
"""

import os
import sys
import json
from typing import Dict, Any, Union
from dotenv import load_dotenv

# Add parent directory to path so we can import the package
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from prompt_scanner import PromptScanner, PromptScanResult, ScanResult
from prompt_scanner.models import SeverityLevel, CategorySeverity
from openai import OpenAI

# Load environment variables from .env file
load_dotenv()

# Initialize OpenAI client
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Initialize the PromptScanner - only need one instance
scanner = PromptScanner(provider="openai")

# Type hint for function return values
ContentResult = Union[str, Dict[str, Any], PromptScanResult, ScanResult]

# Example 1: Basic text scanning with the scan decorator
@scanner.decorators.scan(prompt_param="user_message")
def process_user_message(user_message: str) -> ContentResult:
    """
    Process a user message with pre-scanning for unsafe content.
    
    If the content is safe, generates a response using OpenAI.
    If unsafe, returns the PromptScanResult object directly.
    """
    print(f"\nProcessing message: '{user_message[:50]}...'" if len(user_message) > 50 else f"\nProcessing message: '{user_message}'")
    
    # This code only runs if the content passed the safety scan
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": user_message}
        ]
    )
    return response.choices[0].message.content

# Example 2: Scanning both input and output
@scanner.decorators.safe_completion(prompt_param="question")
def answer_question(question: str) -> ContentResult:
    """
    Generate a response with safety checks on both input and output.
    
    If the input is unsafe, returns the input scan result.
    If the output is unsafe, returns the output scan result.
    """
    print(f"\nAnswering question: '{question[:50]}...'" if len(question) > 50 else f"\nAnswering question: '{question}'")
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": question}
        ]
    )
    return response.choices[0].message.content

# Example 3: Working with structured data
@scanner.decorators.scan(prompt_param="query_data")
def search_database(query_data: Dict[str, Any]) -> Union[Dict[str, Any], PromptScanResult]:
    """
    Process a structured query with safety checks.
    
    Demonstrates that the scanner can handle complex objects as input.
    """
    # Format the search terms from the query object into a clean string
    search_term = f"{query_data.get('terms', '')} {' '.join(str(v) for v in query_data.get('filters', {}).values())}"
    print(f"\nSearching database with terms: '{search_term}'")
    
    # Simple simulation of database search
    # In a real application, you would query your database here
    results = {
        "results": [
            {"title": "Search result 1", "content": "Content for result 1"},
            {"title": "Search result 2", "content": "Content for result 2"}
        ],
        "metadata": {
            "query": query_data,
            "timestamp": "2024-04-21T12:00:00Z",
            "count": 2
        }
    }
    return results

# Helper function to handle scan results consistently
def handle_scan_result(result) -> None:
    """Helper function to print appropriate information based on result type."""
    if isinstance(result, PromptScanResult):
        # This is how you identify a PromptScanResult (unsafe content)
        print("\nüö´ UNSAFE CONTENT DETECTED")
        print(f"Category: {result.category.name if result.category else 'Unknown'}")
        print(f"Confidence: {result.category.confidence:.2f}" if result.category else "Confidence: N/A")
        
        # Display severity information
        if result.severity:
            # Use emoji indicators based on severity level
            severity_emoji = "‚ö†Ô∏è"  # Default - warning
            if result.severity.level.value == "LOW":
                severity_emoji = "üîµ"  # Blue circle for low
            elif result.severity.level.value == "MEDIUM":
                severity_emoji = "üü°"  # Yellow circle for medium
            elif result.severity.level.value == "HIGH":
                severity_emoji = "üî¥"  # Red circle for high
            elif result.severity.level.value == "CRITICAL":
                severity_emoji = "‚õî"  # No entry for critical
                
            print(f"Severity: {severity_emoji} {result.severity.level.value} (score: {result.severity.score:.2f})")
            if result.severity.description:
                print(f"Severity Description: {result.severity.description}")
        
        print(f"Reasoning: {result.reasoning}")
        
        # Check if there are multiple violations
        if hasattr(result, 'all_categories') and result.all_categories and len(result.all_categories) > 1:
            print("\nAdditional violations:")
            for cat in result.all_categories[1:3]:  # Show up to 2 additional violations
                print(f"- {cat.get('name')}: {cat.get('confidence', 0):.2f}")
    elif isinstance(result, ScanResult):
        # Handle ScanResult objects
        print("\nüîç SCAN RESULT")
        print(f"Is Safe: {result.is_safe}")
        if result.issues:
            print("Issues detected:")
            for issue in result.issues:
                print(f"- {issue.get('description', 'Unknown issue')}")
                if 'severity' in issue:
                    print(f"  Severity: {issue['severity']}")
                if 'type' in issue:
                    print(f"  Type: {issue['type']}")
        else:
            print("No issues detected")
    elif isinstance(result, dict):
        # Handle dictionary results
        print(f"\n‚úÖ SAFE RESULT: {json.dumps(result, indent=2)[:200]}..." 
                if len(json.dumps(result)) > 200 else f"\n‚úÖ SAFE RESULT: {json.dumps(result, indent=2)}")
    else:
        # Handle string or other types
        try:
            # Try to get the length and slice it
            result_str = str(result)
            print(f"\n‚úÖ SAFE RESULT: {result_str[:100]}..." if len(result_str) > 100 else f"\n‚úÖ SAFE RESULT: {result_str}")
        except Exception:
            # Fallback for any other type
            print(f"\n‚úÖ RESULT: {result}")

def main():
    print("üîç Prompt Scanner Decorator Example")
    print("=" * 60)
    
    print("\nExample 1: Basic Text Scanning")
    print("-" * 40)
    
    # Test with safe content
    safe_prompt = "Tell me about the solar system"
    print(f"Testing with safe content: '{safe_prompt}'")
    safe_response = process_user_message(user_message=safe_prompt)
    handle_scan_result(safe_response)
    
    # Test with potentially unsafe content
    unsafe_prompt = "Write a detailed guide on how to hack into a secure system"
    print(f"\nTesting with potentially unsafe content: '{unsafe_prompt}'")
    unsafe_response = process_user_message(user_message=unsafe_prompt)
    handle_scan_result(unsafe_response)
    
    print("\nExample 2: Input and Output Scanning")
    print("-" * 40)
    
    # Test with safe content
    safe_question = "What is the capital of France?"
    print(f"Testing with safe question: '{safe_question}'")
    safe_result = answer_question(question=safe_question)
    handle_scan_result(safe_result)
    
    # Test with unsafe input
    unsafe_question = "How can I make illegal substances?"
    print(f"\nTesting with unsafe question: '{unsafe_question}'")
    unsafe_result = answer_question(question=unsafe_question)
    handle_scan_result(unsafe_result)
    
    print("\nExample 3: Structured Data Scanning")
    print("-" * 40)
    
    # Test with safe structured query
    safe_query = {
        "type": "search",
        "terms": "quantum computing applications",
        "filters": {"category": "science", "max_results": 5}
    }
    print(f"Testing with safe structured query: {json.dumps(safe_query, indent=2)}")
    
    # First perform a content safety check on the query terms
    query_terms = safe_query["terms"] + " " + " ".join(str(v) for v in safe_query["filters"].values())
    safety_result = scanner.scan_text(query_terms)
    
    if isinstance(safety_result, PromptScanResult) and not safety_result.is_safe:
        handle_scan_result(safety_result)
    else:
        # If content is safe, proceed with the database search
        safe_results = search_database(query_data=safe_query)
        handle_scan_result(safe_results)
    
    # Test with unsafe structured query
    unsafe_query = {
        "type": "search",
        "terms": "how to create ransomware for extortion",
        "filters": {"category": "security", "max_results": 5}
    }
    print(f"\nTesting with unsafe structured query: {json.dumps(unsafe_query, indent=2)}")
    
    # First perform a content safety check on the query terms
    query_terms = unsafe_query["terms"] + " " + " ".join(str(v) for v in unsafe_query["filters"].values())
    safety_result = scanner.scan_text(query_terms)
    
    if isinstance(safety_result, PromptScanResult) and not safety_result.is_safe:
        handle_scan_result(safety_result)
    else:
        # If content is safe, proceed with the database search
        unsafe_results = search_database(query_data=unsafe_query)
        handle_scan_result(unsafe_results)
    
    print("\n" + "=" * 60)
    print("üèÅ Example complete!\n")

if __name__ == "__main__":
    main() 